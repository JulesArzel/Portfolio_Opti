{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f776d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from portfolio_env import PortfolioEnv\n",
    "from rl_agent import train_rl_agent, evaluate_agent\n",
    "\n",
    "# Step 1: construct env\n",
    "env = PortfolioEnv(features, returns)\n",
    "\n",
    "# Step 2: train\n",
    "model = train_rl_agent(env, timesteps=100_000)\n",
    "\n",
    "# Step 3: evaluate\n",
    "history, infos = evaluate_agent(model, env)\n",
    "\n",
    "# Step 4: plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.title(\"RL Agent Portfolio Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6940d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00_feature_engineering.ipynb\n",
    "\n",
    "prices = load_price_data(...)   # shape (T, n_assets)\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "# build regime + markowitz weights\n",
    "hmm_regimes = get_hmm_regimes(prices)\n",
    "predicted_regimes = predict_future_regimes(prices)\n",
    "markowitz_weights = get_markowitz_weights(prices)\n",
    "\n",
    "# feature vector for RL\n",
    "features_df = pd.concat([\n",
    "    regime_one_hot(hmm_regimes),\n",
    "    regime_one_hot(predicted_regimes),\n",
    "    markowitz_weights\n",
    "], axis=1).dropna()\n",
    "\n",
    "# align returns\n",
    "features_df = features_df.loc[returns.index]\n",
    "returns_df = returns.loc[features_df.index]\n",
    "\n",
    "# Then pass to env + model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
